{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733a3ee8",
   "metadata": {},
   "source": [
    "# Advanced Python: BYONN, Part II\n",
    "**B**uild **Y**our **O**wn **N**eural **N**etwork\n",
    "\n",
    "<center>\n",
    "<img src=\"./pictures/cajal_glial.jpg\" style=\"width:480px;height:626px;\">\n",
    "<br>\n",
    "<i>Glial cells of the cerebral cortex of a child (Santiago Ramón y Cajal, 1904)</i>\n",
    "</center>\n",
    "\n",
    "Recall that in the last session, we saw how to build a simple neural network using `Layer`s and `Module` objects in `pytorch`. Here, we will build a similar network to classify the MNIST dataset.\n",
    "\n",
    "References:\n",
    "- [Feed-Forward Neural Network (FFNN) — PyTorch](https://medium.com/@carlosrodrigo.coelho/feed-forward-neural-network-ffnn-pytorch-d5d9759f53d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1c38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7de7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set\n",
    "mnist_train = MNIST(root='data/', train=True, download=True, \n",
    "                    transform=transforms.ToTensor()) \n",
    "                    # needs to transform img>tensor\n",
    "# load testing set\n",
    "mnist_test = MNIST(root='data/', train=False, download=True,\n",
    "                   transform=transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3e5a2",
   "metadata": {},
   "source": [
    "### Warm up:\n",
    "`mnist_train` and `mnist_test` are iterables (you can index them like a list). Each item is a tuple of (image, label) pair.\n",
    "\n",
    "1. Figure out what's the shape of the images.\n",
    "2. How many images are there in the training set? How about the testing set?\n",
    "3. How many classes are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a569c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 28, 28]), label: 5\n"
     ]
    }
   ],
   "source": [
    "(image, label) = mnist_train[0] # get first image and label\n",
    "print(f'image shape: {image.shape}, label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnist_train), len(mnist_test)  # number of training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c9dd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the following variables to match the dataset\n",
    "n_inputs = image.shape[0] * image.shape[1] * image.shape[2]  # 1*28*28 for MNIST\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a52ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# use pytorch DataLoaders to automatically load/feed data during train/test\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f275d0",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "Define a neural network object from `pytorch` that consists of 1 hidden layer with 56 neurons and ReLU activation function. Importantly, the network should be able to take in images and output a vector of probabilities for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "353cab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1 = nn.Linear(n_inputs, n_hidden) # first layer: inputs to hidden\n",
    "        self.relu = nn.ReLU() # activation function (ReLU)\n",
    "        self.l2 = nn.Linear(n_hidden, n_classes) # hidden to output\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d712e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 56\n",
    "\n",
    "# initialize instance    \n",
    "model = NeuralNet(n_inputs, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac9f78",
   "metadata": {},
   "source": [
    "With the neural network defined, we can now train it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ef29391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "# makes sense to use cross entropy loss because we are training for discrete\n",
    "# category classifications (also could write our own loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6221aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, step 100/600, loss = 0.8391\n",
      "epoch 1/10, step 200/600, loss = 0.5677\n",
      "epoch 1/10, step 300/600, loss = 0.3694\n",
      "epoch 1/10, step 400/600, loss = 0.4016\n",
      "epoch 1/10, step 500/600, loss = 0.3343\n",
      "epoch 1/10, step 600/600, loss = 0.3748\n",
      "epoch 2/10, step 100/600, loss = 0.2895\n",
      "epoch 2/10, step 200/600, loss = 0.3546\n",
      "epoch 2/10, step 300/600, loss = 0.2983\n",
      "epoch 2/10, step 400/600, loss = 0.2338\n",
      "epoch 2/10, step 500/600, loss = 0.3906\n",
      "epoch 2/10, step 600/600, loss = 0.3491\n",
      "epoch 3/10, step 100/600, loss = 0.2763\n",
      "epoch 3/10, step 200/600, loss = 0.2277\n",
      "epoch 3/10, step 300/600, loss = 0.1851\n",
      "epoch 3/10, step 400/600, loss = 0.2308\n",
      "epoch 3/10, step 500/600, loss = 0.2759\n",
      "epoch 3/10, step 600/600, loss = 0.1857\n",
      "epoch 4/10, step 100/600, loss = 0.2194\n",
      "epoch 4/10, step 200/600, loss = 0.1231\n",
      "epoch 4/10, step 300/600, loss = 0.2247\n",
      "epoch 4/10, step 400/600, loss = 0.1230\n",
      "epoch 4/10, step 500/600, loss = 0.1999\n",
      "epoch 4/10, step 600/600, loss = 0.1656\n",
      "epoch 5/10, step 100/600, loss = 0.0905\n",
      "epoch 5/10, step 200/600, loss = 0.3475\n",
      "epoch 5/10, step 300/600, loss = 0.0879\n",
      "epoch 5/10, step 400/600, loss = 0.0740\n",
      "epoch 5/10, step 500/600, loss = 0.1134\n",
      "epoch 5/10, step 600/600, loss = 0.1777\n",
      "epoch 6/10, step 100/600, loss = 0.1837\n",
      "epoch 6/10, step 200/600, loss = 0.1267\n",
      "epoch 6/10, step 300/600, loss = 0.1835\n",
      "epoch 6/10, step 400/600, loss = 0.1156\n",
      "epoch 6/10, step 500/600, loss = 0.2642\n",
      "epoch 6/10, step 600/600, loss = 0.1320\n",
      "epoch 7/10, step 100/600, loss = 0.1376\n",
      "epoch 7/10, step 200/600, loss = 0.1353\n",
      "epoch 7/10, step 300/600, loss = 0.1373\n",
      "epoch 7/10, step 400/600, loss = 0.1172\n",
      "epoch 7/10, step 500/600, loss = 0.1880\n",
      "epoch 7/10, step 600/600, loss = 0.0715\n",
      "epoch 8/10, step 100/600, loss = 0.0540\n",
      "epoch 8/10, step 200/600, loss = 0.2421\n",
      "epoch 8/10, step 300/600, loss = 0.0791\n",
      "epoch 8/10, step 400/600, loss = 0.0557\n",
      "epoch 8/10, step 500/600, loss = 0.0855\n",
      "epoch 8/10, step 600/600, loss = 0.2012\n",
      "epoch 9/10, step 100/600, loss = 0.0975\n",
      "epoch 9/10, step 200/600, loss = 0.0909\n",
      "epoch 9/10, step 300/600, loss = 0.0911\n",
      "epoch 9/10, step 400/600, loss = 0.0818\n",
      "epoch 9/10, step 500/600, loss = 0.1311\n",
      "epoch 9/10, step 600/600, loss = 0.0437\n",
      "epoch 10/10, step 100/600, loss = 0.0787\n",
      "epoch 10/10, step 200/600, loss = 0.0984\n",
      "epoch 10/10, step 300/600, loss = 0.0652\n",
      "epoch 10/10, step 400/600, loss = 0.0465\n",
      "epoch 10/10, step 500/600, loss = 0.0557\n",
      "epoch 10/10, step 600/600, loss = 0.0825\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_loader) # number of batches in training set\n",
    "\n",
    "# for storing lost history\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # reshape images to (batch_size, input_size)\n",
    "        # eg: 256,1,28,28 -> 256,784\n",
    "        images = images.reshape(-1, 28**2)\n",
    "        labels = labels\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # saving the loss at current pass to log\n",
    "        epoch_loss.append(loss.detach().numpy()) # save value\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # set gradients to zero in every batch\n",
    "        loss.backward()     # backpropagation\n",
    "        optimizer.step()   # update weights\n",
    "\n",
    "        if (i+1) % 100 == 0: # print every 100 steps\n",
    "            print(f'epoch {epoch+1}/{n_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "    \n",
    "    # save mean loss of the epoch\n",
    "    loss_hist.append(np.mean(np.array(epoch_loss)))\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be0c96fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x304192490>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bafa3da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 96.53\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28**2)  # 100,1,28,28 -> 100,784\n",
    "        labels = labels          # 100,1 -> 100\n",
    "        outputs = model(images)  # 100,10\n",
    "\n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs, 1) # 1 is the dimension\n",
    "        n_samples += labels.shape[0] # number of samples in the current batch\n",
    "        n_correct += (predictions == labels).sum().item()  # number of correct predictions\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples  # accuracy\n",
    "    print(f'accuracy = {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pni_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
